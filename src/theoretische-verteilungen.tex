\chapter{Theoretische Verteilungen}
Theoretische Verteilungen bilden den Übergang von der deskriptiven zur induktiven Statistik und sind eng mit den Prinzipien der Wahrscheinlichkeitsrechnung verbunden.
Im Gegensatz zu empirischen Verteilungen, die auf der Beobachtung und Zusammenfassung von tatsächlich erhobenen Daten basieren, sind theoretische Verteilungen mathematische Modelle, die die Wahrscheinlichkeiten von Ergebnissen beschreiben, basierend auf bestimmten Annahmen oder Theorien.
\newline \newline
In der Praxis stellen theoretische Verteilungen idealisierte oder vereinfachte Darstellungen der Realität dar und dienen als nützliche Werkzeuge für die Durchführung von statistischen Tests und Schätzungen.
Sie ermöglichen es uns, Hypothesen zu prüfen und Schlussfolgerungen über eine Population basierend auf einer Stichprobe zu ziehen.
Einige der bekanntesten theoretischen Verteilungen sind die Normalverteilung, die Binomialverteilung und die Poisson-Verteilung, die jeweils verschiedene Arten von Daten und Situationen modellieren.
\newline \newline
Bei den theoretischen Verteilungen heißen die Merkmalsausprägungen \textbf{Ereignisse} und die relativen Häufigkeiten \textbf{Wahrscheinlichkeiten.}


\section{Zufallsvariablen}

Zufallsvariablen sind ein grundlegendes Konzept in der Wahrscheinlichkeitstheorie und Statistik. Formal betrachtet, ist eine Zufallsvariable eine messbare Funktion $X$, die jedem Ergebnis $\omega$ aus einem Wahrscheinlichkeitsraum $(\Omega, \mathcal{F}, P)$ eine reelle Zahl zuordnet. Das bedeutet, dass die Zufallsvariable $X$ den Ergebnissen aus dem Ereignisraum $\Omega$ bestimmte Werte zuweist, die wir messen oder beobachten können.

\textit{Zufallsvariablen können in zwei Haupttypen unterteilt werden: diskrete und stetige Zufallsvariablen.}

\textbf{Diskrete Zufallsvariablen} haben eine Zählmenge von möglichen Ergebnissen. Beispiele hierfür sind Würfelwürfe, die Anzahl der Münzwürfe bis zum ersten Kopf oder die Anzahl der Personen, die an einem bestimmten Tag in einem Geschäft einkaufen.

\textbf{Stetige Zufallsvariablen} hingegen können jeden Wert innerhalb eines bestimmten Bereichs annehmen, wie zum Beispiel die Zeit, die eine Person auf einen Bus wartet, oder das Gewicht einer zufällig ausgewählten Person.

\textit{Ein grundlegendes Beispiel für eine diskrete Zufallsvariable ist das Werfen eines fairen Würfels.}

In diesem Fall ist der Wahrscheinlichkeitsraum gegeben durch $\Omega = \{1, 2, 3, 4, 5, 6\}$, das sind die möglichen Ergebnisse (Augenzahlen) beim Würfeln. Wir definieren die Zufallsvariable $X$ als die Augenzahl, die beim Würfeln auftritt. Die Wahrscheinlichkeit für jedes Ergebnis ist gleich und beträgt $\frac{1}{6}$, da wir einen fairen Würfel verwenden.

Die Verteilung dieser Zufallsvariablen kann in der folgenden Tabelle dargestellt werden:

\begin{center}
    \begin{tabular}{c|c|c|c|c|c|c}
        $x_i$        & 1             & 2             & 3             & 4             & 5             & 6             \\
        \hline
        $P(X = x_i)$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ \\
    \end{tabular}
\end{center}

Diese Tabelle zeigt uns die Wahrscheinlichkeit dafür, dass die Zufallsvariable $X$ einen bestimmten Wert annimmt.
Es ist wichtig zu verstehen, dass diese Wahrscheinlichkeiten theoretische Wahrscheinlichkeiten sind, die auf der Annahme basieren, dass der Würfel fair ist.


\section{Erläuterungen am Beispiel der diskreten Gleichverteilung}

Eine diskrete Zufallsvariable $X$ kann verschiedene Werte annehmen, die wir als $x_1, x_2, \dots, x_k$ bezeichnen.
Jeder dieser Werte hat eine zugeordnete Wahrscheinlichkeit, die wir als $p_1, p_2, \dots, p_k$ bezeichnen.
So kann die Verteilung der Zufallsvariable $X$ durch die folgende Tabelle dargestellt werden:

\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        $x_i$ & & $x_1$ & $x_2$ & $\cdots$ & $x_k$ \\
        \hline
        $p_i$ & & $p_1$ & $p_2$ & $\cdots$ & $p_k$ \\
        \hline
    \end{tabular}
\end{center}

Hierbei ist $p_i = P(X=x_i)$ die Wahrscheinlichkeit, dass die Zufallsvariable $X$ den Wert $x_i$ annimmt.
Die Funktion, die jedem möglichen Wert $x_i$ der Zufallsvariable die zugehörige Wahrscheinlichkeit $p_i$ zuweist, nennen wir die \textit{Wahrscheinlichkeitsfunktion} oder \textit{Wahrscheinlichkeitsverteilung}.

Ein besonders wichtiger Wert in der Verteilung ist der \textit{Modus} $x_D$, das ist der Wert $x_i$ mit der höchsten Wahrscheinlichkeit $p_i$.
In einem Säulendiagramm, das die Werte $x_i$ gegen ihre Wahrscheinlichkeiten $p_i$ aufträgt, ist der Modus die Säule mit der größten Höhe.
Dieses Diagramm ist eine hilfreiche Visualisierung, die einen schnellen Überblick über die Verteilung ermöglicht.

Eine andere Art, eine diskrete Verteilung darzustellen, ist die \textit{Verteilungsfunktion}, die wir als $F_i = P(X \leq x_i)$ definieren.
Das ist die Wahrscheinlichkeit, dass die Zufallsvariable einen Wert annimmt, der kleiner oder gleich $x_i$ ist.
Die Verteilungsfunktion bildet eine Treppenfunktion, wenn sie als Diagramm dargestellt wird, mit Stufen an den Werten $x_i$ und Höhen entsprechend den kumulierten Wahrscheinlichkeiten $F_i$.

\subsection{Allgemeingültige Rechenregeln zu kumulierten diskreten Wahrscheinlichkeiten}
Die folgenden Formeln repräsentieren verschiedene Wahrscheinlichkeiten, die von den Grenzwerten $x_i$ und $x_j$ und der Verteilungsfunktion $F(x)$ abhängen.
\begin{center}
    \begin{tabular}{|l|l|p{6cm}|}
        \hline
        \textbf{Wahrscheinlichkeit} & \textbf{Formel}                                   & \textbf{Beschreibung}                                                                   \\
        \hline
        $P(X < x_i)$                & $F_i - P(x_i)=F(x_{i-1})$                         & Wahrscheinlichkeit, dass $X$ kleiner als $x_i$ ist                                      \\
        \hline
        $P(X > x_i)$                & $1- F(x)$                                         & Wahrscheinlichkeit, dass $X$ größer als $x_i$ ist                                       \\
        \hline
        $P(X \geq x_i)$             & $1- F(x) + P(x_i) = 1 - F(x_{i-1})$               & Wahrscheinlichkeit, dass $X$ größer oder gleich $x_i$ ist                               \\
        \hline
        $P(x_j < X < x_i)$          & $F_i - F_j$                                       & Wahrscheinlichkeit, dass $X$ größer als $x_j$ und kleiner als $x_i$ ist                 \\
        \hline
        $P(x_j \leq X \leq x_i)$    & $F_i - F_j + P(x_j) = F_i - F_{j-1}$              & Wahrscheinlichkeit, dass $X$ größer oder gleich $x_j$ und kleiner oder gleich $x_i$ ist \\
        \hline
        $P(x_j \leq X < x_i)$       & $F_i - P(x_i) - F_j + P(x_j) = F_{i-1} - F_{j-1}$ & Wahrscheinlichkeit, dass $X$ größer oder gleich $x_j$ und kleiner als $x_i$ ist         \\
        \hline
        $P(x_j < X < x_i)$          & $F_i - P(x_i) - F_j = F_{i-1} - F_j$              & Wahrscheinlichkeit, dass $X$ größer als $x_j$ und kleiner als $x_i$ ist                 \\
        \hline
    \end{tabular}
\end{center}


\section{Lage- und Streumaße}

\subsection{Erwartungswert}
Der Erwartungswert ($E(X)$) ist ein grundlegendes Maß für die zentrale Tendenz einer Wahrscheinlichkeitsverteilung.
Es handelt sich um den gewichteten Durchschnitt der möglichen Ausprägungen einer Zufallsvariable, wobei die Gewichte durch die Wahrscheinlichkeiten gegeben sind.
Formelhaft ausgedrückt wird der Erwartungswert berechnet durch die Summe der Produkte der Werte $x_i$ und deren Wahrscheinlichkeiten $p_i$:

\[
    E(X) = \sum_{i=1}^{n} x_i \cdot p_i
\]

\subsection{Varianz}
Die Varianz ($V(X)$ oder $\sigma^2$) ist ein Maß für die Streuung oder die Variation in einer Reihe von Zufallsvariablen.
Es misst, wie weit die Zahlen in dem Datensatz voneinander entfernt sind.
Die Varianz wird berechnet durch den Erwartungswert der quadrierten Abweichungen vom Erwartungswert:

\[
    V(X) = E(X^2) - E(X)^2
\]

\subsection{Standardabweichung}
Die Standardabweichung ($\sigma$) ist die Quadratwurzel der Varianz.
Sie ist ein nützliches Maß, da sie die Streuung der Werte um den Erwartungswert in derselben Einheit wie die Werte selbst angibt:

\[
    \sigma = \sqrt{V(X)}
\]

\ex{Lage- und Streumaße am Münzwurf}{
    Betrachten wir das Beispiel eines Münzwurfs. Wenn die Münze auf \textit{Zahl} landet, gewinnen wir 5 Euro, und wenn sie auf \textit{Kopf} landet, verlieren wir 10 Euro.

    \[
        \begin{array}{|c|c|}
            \hline
            x_i & p_i \\
            \hline
            5   & 0.5 \\
            -10 & 0.5 \\
            \hline
        \end{array}
    \]

    Wir berechnen den Erwartungswert $E(X)$ als die Summe der Produkte von $x_i$ und $p_i$:

    \[
        E(X) = \sum x_i \cdot p_i = 5 \cdot 0.5 + (-10) \cdot 0.5 = 2.5 - 5 = -2.5 \, \text{Euro}
    \]

    Wir berechnen den Erwartungswert der Quadrate, $E(X^2)$:

    \[
        E(X^2) = \sum x_i^2 \cdot p_i = 5^2 \cdot 0.5 + (-10)^2 \cdot 0.5 = 12.5 + 50 = 62.5
    \]

    Daraus können wir die Varianz $V(X)$ berechnen:

    \[
        V(X) = E(X^2) - [E(X)]^2 = 62.5 - (-2.5)^2 = 62.5 - 6.25 = 56.25
    \]

    Schließlich berechnen wir die Standardabweichung als Quadratwurzel der Varianz:

    \[
        \sqrt{V(X)} = \sqrt{56.25} = 7.5 \, \text{Euro}
    \]

    Diese Werte zeigen uns, dass wir im Durchschnitt 2.5 Euro verlieren und dass die Standardabweichung 7.5 Euro beträgt, was eine erhebliche Schwankung um den Erwartungswert darstellt.
}


\section{Die Hypergeometrische Verteilung}

Die hypergeometrische Verteilung ist ein wichtiges Modell in der Wahrscheinlichkeitstheorie und beschreibt die Wahrscheinlichkeit für eine bestimmte Anzahl an Erfolgen in einer Ziehung ohne Zurücklegen aus einer endlichen Population.

Der Begriff \textit{Hypergeometrisch} stammt aus der Mathematik und bezieht sich auf die Beziehung zwischen mehreren geometrischen Objekten.
In diesem Kontext bezieht es sich auf die Beziehung zwischen der Anzahl der Erfolge, der Anzahl der Ziehungen und der Größe der Population.

Die hypergeometrische Verteilung wird unter den folgenden Bedingungen verwendet:
\begin{itemize}
    \item Die Population oder Grundgesamtheit ist endlich.
    \item Es gibt nur zwei mögliche Ausgänge für jedes Ereignis (Erfolg oder Misserfolg).
    \item Die Ziehungen sind ohne Zurücklegen, was bedeutet, dass jedes gezogene Objekt nicht in die Population zurückkehrt.
\end{itemize}

Die Wahrscheinlichkeitsfunktion der hypergeometrischen Verteilung wird in der Form eines Quotienten von Binomialkoeffizienten dargestellt.
Diese Form gibt die Anzahl der Möglichkeiten an, $k$ Erfolge aus $K$ möglichen zu ziehen und $n-k$ Misserfolge aus $N-K$ möglichen zu ziehen, geteilt durch die Gesamtzahl der Möglichkeiten, $n$ Objekte aus $N$ zu ziehen.
Die Formel ist:

\[
    P(X=k) = \frac{{K \choose k} \cdot {{N-K} \choose {n-k}}}{{N \choose n}}
\]

\ex{Faule Äpfel aus einem Korb ziehen}{
    Wir betrachten ein Experiment, bei dem drei Äpfel aus einem Korb mit 10 Äpfeln, darunter 3 faule Äpfel, gezogen werden.
    Die Äpfel werden nach dem Ziehen nicht zurückgelegt.
    Wir sind interessiert an der Wahrscheinlichkeit, bei allen drei Zügen einen faulen Apfel zu ziehen.

    In dieser Situation haben wir eine Gesamtpopulation von $N=10$ Äpfeln, wovon $K=3$ Äpfel faul sind.
    Die Anzahl der Ziehungen beträgt $n=3$.

    Die Wahrscheinlichkeit, bei allen drei Zügen einen faulen Apfel zu ziehen, ergibt sich dann durch Einsetzen dieser Werte in die Formel für die hypergeometrische Verteilung:

    \[
        P(X=3) = \frac{{K \choose 3} \cdot {{N-K} \choose {n-3}}}{N \choose n} = \frac{{3 \choose 3} \cdot {{10-3} \choose {3-3}}}{10 \choose 3} = 0.008\bar{3} = 0.8\bar{3}\%
    \]

    Dies bedeutet, dass die Wahrscheinlichkeit, dass alle drei gezogenen Äpfel faul sind, bei etwa 0.8\bar{3}\% liegt.

    Der Erwartungswert $\mu$ für die Anzahl der faulen Äpfel, die wir ziehen, berechnet sich allgemein durch
    \[
        \mu = E(X) = \frac{n \cdot K}{N} = \frac{3 \cdot 3}{10} = 0.9
    \]
    Dies bedeutet, dass wir im Durchschnitt 0.9 faule Äpfel ziehen würden, wenn wir das Experiment viele Male wiederholen würden.
}


\section{Die Binomialverteilung}

Die Binomialverteilung ist eine diskrete Wahrscheinlichkeitsverteilung, die sich aus der Betrachtung einer Serie von unabhängigen und gleichverteilten Zufallsexperimenten ableitet.
Jedes dieser Experimente, auch als Bernoulli-Versuche bezeichnet, hat genau zwei mögliche Ergebnisse: \textit{Erfolg} (mit einer Wahrscheinlichkeit $p$) und \textit{Fehlschlag} (mit einer Wahrscheinlichkeit $1-p$).
\newline \newline
Die Binomialverteilung gibt nun die Wahrscheinlichkeit dafür an, dass bei $n$ solcher unabhängigen Experimente genau $k$ Erfolge auftreten.
Die Herleitung der Binomialverteilung folgt aus der Kombinatorik: Die Anzahl der Möglichkeiten, genau $k$ Erfolge in $n$ Versuchen zu erzielen, entspricht der Anzahl der Kombinationen von $n$ Elementen, die $k$ enthalten, multipliziert mit der Wahrscheinlichkeit, genau diese Kombination zu erhalten.

Mathematisch ausgedrückt lässt sich die Wahrscheinlichkeit $P(X=k)$, bei $n$ Versuchen genau $k$ Erfolge zu erzielen, durch die Binomialverteilung wie folgt darstellen:

\[
    P(X=k) = {n \choose k} p^k (1-p)^{n-k}
\]

Eine andere Möglichkeit, eine diskrete Wahrscheinlichkeitsverteilung zu beschreiben, ist die Angabe ihrer Wahrscheinlichkeitsfunktion, welche die Menge aller Paare $(k, P(X=k))$, also:
\[
    P(k) \coloneqq
    \begin{cases}
        \mathbb{N} &\rightarrow [0;1] \\
        k &\rightarrow {n \choose k} p^k (1-p)^{n-k}
    \end{cases}
\]

Die Binomialverteilung hat eine breite Anwendung in der Statistik und in verschiedenen Fachbereichen.
Beispiele dafür sind die Modellierung von Zufallsexperimenten, in denen die Anzahl der Erfolge in einer festen Anzahl von unabhängigen Bernoulli-Versuchen von Interesse ist, oder die Modellierung von Wahrscheinlichkeiten in der Qualitätskontrolle, im Risikomanagement und in der genetischen Forschung.

\subsection{Maßzahlen und Verteilungsfunktion}
Ein wesentlicher Vorteil der Binomialverteilung liegt in der Einfachheit ihrer Maßzahlen.
Insbesondere lassen sich der Erwartungswert und die Varianz direkt aus den Parametern der Verteilung berechnen.

Der Erwartungswert $E(X)$ einer Binomialverteilung ist einfach das Produkt aus der Anzahl der Versuche $n$ und der Erfolgswahrscheinlichkeit $p$:

\[
    E(X) = n \cdot p
\]

Dies ergibt sich direkt aus der Definition des Erwartungswertes als gewichteter Durchschnitt aller möglichen Werte, wobei die Gewichtung durch die jeweilige Wahrscheinlichkeit erfolgt.
Bei der Binomialverteilung ist die Wahrscheinlichkeit für jeden Versuch gleich $p$, daher ergibt sich der Erwartungswert direkt als Produkt aus der Anzahl der Versuche und der Erfolgswahrscheinlichkeit.

Ähnlich verhält es sich mit der Varianz $V(X)$, die die erwartete quadratische Abweichung vom Erwartungswert misst.
Für eine Binomialverteilung kann die Varianz einfach als

\[
    V(X) = n \cdot p \cdot (1-p)
\]

berechnet werden.
Dies ergibt sich aus der Tatsache, dass die Varianz einer Binomialverteilung die Summe der Varianzen der einzelnen Bernoulli-Versuche ist, von denen jeder eine Varianz von $p \cdot (1-p)$ hat.

Die kumulative Verteilungsfunktion $F(x)$ einer diskreten Zufallsvariablen ist definiert als die Summe der Wahrscheinlichkeiten für alle Werte kleiner oder gleich $x$.
In der Praxis bedeutet dies, dass man die Wahrscheinlichkeiten aller einzelnen Ereignisse bis zu einem bestimmten Wert $x$ addiert.
Für die Binomialverteilung ergibt sich daher:

\[
    F(x) = \sum_{i=0}^{x} P(X=i)
\]

\subsection{Gegenereignis}

Das Konzept des \textit{Gegenereignisses} spielt eine wichtige Rolle in der Wahrscheinlichkeitstheorie.
Das Gegenereignis zu einem Ereignis $A$ in einem Wahrscheinlichkeitsraum ist das Ereignis, dass $A$ \textit{nicht} eintritt.
Es wird oft mit $\overline{A}$ oder $A'$ bezeichnet.

Im Kontext der Binomialverteilung kann das Gegenereignis besonders nützlich sein.
Angenommen, wir sind an der Wahrscheinlichkeit $P(X \geq k)$ für eine Zufallsvariable $X$ interessiert, die binomialverteilt ist.
Direkt diese Wahrscheinlichkeit zu berechnen kann umständlich sein, da es das Aufsummieren der Wahrscheinlichkeiten für viele verschiedene Ergebnisse erfordert.

An dieser Stelle kann das Gegenereignis ins Spiel kommen.
Beachten wir, dass das Ereignis $\{X \geq k\}$ das Gegenereignis zu $\{X < k\}$ ist.
Mit anderen Worten, entweder $X$ ist größer oder gleich $k$, oder $X$ ist kleiner als $k$; es gibt keine andere Möglichkeit.
Daher gilt:
\[
    P(X \geq k) = 1 - P(X < k).
\]

Da $P(X < k)$ die Wahrscheinlichkeit ist, dass $X$ einen Wert annimmt, der kleiner ist als $k$, müssen wir nur die Wahrscheinlichkeiten für weniger Ergebnisse addieren, was oft einfacher ist.

Es ist wichtig zu beachten, dass dieses Prinzip nicht nur für die Binomialverteilung gilt, sondern für alle Wahrscheinlichkeitsverteilungen.
Die Verwendung von Gegenereignissen ist eine mächtige Technik in der Wahrscheinlichkeitstheorie und kann oft dazu beitragen, Berechnungen erheblich zu vereinfachen.

\section{Poissonverteilung}

Die \textit{Poissonverteilung} ist eine diskrete Wahrscheinlichkeitsverteilung, die Ereignisse beschreibt, die selten eintreten und unabhängig von der Zeit sind.
Sie ist nach dem französischen Mathematiker Siméon Denis Poisson benannt, der sie im Kontext der Modellierung der Anzahl der Todesfälle infolge von Pferdetritten in der Preußischen Armee eingeführt hat.

\subsection{Herleitung und Beziehung zur Binomialverteilung}

Die Poisson-Verteilung kann als Grenzfall der Binomialverteilung für $n \rightarrow \infty$ und $np = \lambda$ (eine feste Zahl) aufgefasst werden.
Dies entspricht der Situation, in der wir eine sehr große Anzahl von unabhängigen Bernoulli-Versuchen durchführen, bei denen die Wahrscheinlichkeit für \textit{Erfolg} sehr klein, die durchschnittliche Anzahl von Erfolgen jedoch endlich ist.

\subsection{Wahrscheinlichkeitsfunktion}

Die Wahrscheinlichkeitsfunktion der Poisson-Verteilung ist gegeben durch:

\[
    P(X = k) = \frac{\mu^k}{k!}e^{-\mu},
\]

für $k \in \mathbb{N}_0 = 0, 1, 2, \dots$.
Hierbei ist $\mu$ der Erwartungswert und die Varianz der Verteilung, und $e$ ist die Basis des natürlichen Logarithmus.

\ex{Berechnung der Wahrscheinlichkeit von Website-Besuchen}{
    Angenommen, wir haben eine Website mit durchschnittlich 5 Besuchern pro Stunde.
    \newline
    Dies ist unser Erwartungswert $\mu = 5$.
    \newline
    Wir möchten die Wahrscheinlichkeit berechnen, dass genau 7 Besucher in einer bestimmten Stunde auf die Website kommen.
    \newline
    \newline
    $P(X=k)$ die Wahrscheinlichkeit, dass $k$ Besucher in einer Stunde auf die Website kommen. \newline
    $\mu$ ist die durchschnittliche Anzahl von Besuchern pro Stunde. \newline
    $k$ ist die Anzahl der Ereignisse, für die wir die Wahrscheinlichkeit berechnen möchten (In diesem Fall $7$ Besucher). \newline
    $e$ ist die Basis des natürlichen Logarithmus (ungefähr 2.71828). \newline
    \newline
    Wir können die Wahrscheinlichkeit für $k=7$ Besucher in einer Stunde wie folgt berechnen:
    \[
        P(X=7) = \frac{5^7}{7!}e^{-5} \approx 0.104
    \]
}

\subsection{Erwartungswert und Varianz}

Für die Poisson-Verteilung gilt, dass der Erwartungswert und die Varianz gleich sind, das heißt $E(X) = \text{Var}(X) = \mu$.
Dies liegt an der Eigenschaft des Poisson-Prozesses, bei dem die Ereignisse unabhängig voneinander auftreten, mit einer festen durchschnittlichen Rate $\lambda$.

\subsection{Poisson-Prozess}

Die Poisson-Verteilung wird oft verwendet, um den \textit{Poisson-Prozess} zu modellieren, ein Modell für eine Serie von Ereignissen, die unabhängig mit einer konstanten durchschnittlichen Rate auftreten.
Beispiele für solche Prozesse sind die Anzahl von Anrufen in einem Callcenter in einem bestimmten Zeitraum oder die Anzahl von Fehlern in einem Computercode.

Die Poisson-Verteilung ist in vielen Bereichen von Bedeutung, einschließlich der Warteschlangentheorie, der Zuverlässigkeitstheorie und der Ereignisdatenanalyse.